{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier which let's you predict if tweet is positive or negative.\n"
      ],
      "metadata": {
        "id": "3c62H99UoBgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import requests\n",
        "import nltk\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from sklearn import tree\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-2Wwsz1xn5UR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqGDrcYjHoWn"
      },
      "outputs": [],
      "source": [
        "brexit_df = pd.read_csv(\"/content/brexitKlasa.csv\", encoding='utf-8')\n",
        "brexit_df = brexit_df.drop_duplicates(subset=['text'])\n",
        "brexit = brexit_df['text'].tolist()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"rt\",\" \", text)\n",
        "    text = re.sub(\"#[A-Za-z0-9_]+\", \" \", text)\n",
        "    text = re.sub(\"@[A-Za-z0-9_]+\", \" \", text)\n",
        "    text = re.sub('(http|https):\\/\\/[a-z\\.]*\\/[a-zA-Z0-9]*', '', text)\n",
        "    text = re.sub(\"n/\",\" \", text)\n",
        "    text = re.sub(\"[?!;(),.:'+-=]\", \" \", text)\n",
        "    text = re.sub(\"[0-9]+\",\" \", text)\n",
        "    text = re.sub(\"'\",\" \", text)\n",
        "    text = re.sub(\"'\",\"\",text)\n",
        "    text = re.sub(\"&\",\" \", text)\n",
        "    text = re.sub(\"…\",\" \", text)\n",
        "    text = re.sub(\"-\",\" \", text)\n",
        "    text = re.sub(\"%\",\" \", text)\n",
        "    text = re.sub('\"',\" \", text)\n",
        "    text = re.sub('@',\" \", text)\n",
        "    text = re.sub('�',\" \", text)\n",
        "    text = re.sub(\"[ ]+\",\" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "url = \"https://www.textfixer.com/tutorials/common-english-words.txt\"\n",
        "response = requests.get(url)\n",
        "stopwords = response.text.split(\",\")\n",
        "stopwords = stopwords + ['htt', 's', 'c', 'y', 'j', 't', \"it’s\", 'https', '', 'h', 'w', 'v', 'u', 'k', \"won't\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_brexit = [clean_text(x) for x in brexit]\n",
        "brexit_sw = []\n",
        "for i in clean_brexit:\n",
        "  nowy_tweet = []\n",
        "  for j in i.split(\" \"):\n",
        "    if j not in stopwords2:\n",
        "      nowy_tweet.append(j)\n",
        "      n = \" \".join(nowy_tweet)\n",
        "  brexit_sw.append(n)\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stem_brexit = []\n",
        "for i in brexit_sw:\n",
        "  temp = []\n",
        "  for j in i.split(\" \"):\n",
        "    temp.append(stemmer.stem(j))\n",
        "  stem_brexit.append(\" \".join(temp))"
      ],
      "metadata": {
        "id": "fCx8IBu0uuEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = TfidfVectorizer(stop_words='english')\n",
        "x=v.fit_transform(stem_brexit)\n",
        "df = pd.DataFrame(x.toarray(), columns=v.get_feature_names())\n",
        "\n",
        "brexit_df = brexit_df.reset_index()\n",
        "brexit_df = brexit_df.drop(columns=['index'])\n",
        "df1 = pd.merge(df, brexit_df, left_index=True, right_index=True, how='left')\n",
        "df1 = df1.drop(columns=['screenName', 'text'])\n",
        "\n",
        "X = df1.iloc[:, :-1].values\n",
        "Y = df1.iloc[:, -1].values\n",
        "min_max_scaler = MinMaxScaler()\n",
        "X_minmax = min_max_scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "kP_5h540qUNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_minmax, df1['klasa'].values , test_size=0.30, random_state=0)\n",
        "model_tree = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
        "model_tree.fit(X_train, y_train)\n",
        "y_pred = model_tree.predict(X_test)"
      ],
      "metadata": {
        "id": "F_hfWeU2LxqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = [100, 100])\n",
        "tree.plot_tree(model_tree, fontsize=60, feature_names = v.get_feature_names(), max_depth= 5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YDzpckxo7RW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier accuracy score. Which class is predicted better? "
      ],
      "metadata": {
        "id": "ikDVw38Y2efM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calssification accuracy is: \", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "_Si31crV2qib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def whichClass():\n",
        "  mat = confusion_matrix(y_test, y_pred)\n",
        "  c = mat.diagonal()/mat.sum(axis=1)\n",
        "  n = c[0]\n",
        "  p = c[1]\n",
        "  if p > n:\n",
        "    print('Higher accuracy for \"pozytywna\" class.')\n",
        "  elif p < n:\n",
        "    print('Higher accuracy for \"negatywna\" class.')\n",
        "  else:\n",
        "    print('None of two classes has higher accuracy.')\n",
        "\n",
        "whichClass()"
      ],
      "metadata": {
        "id": "3Mgzo5Qz25M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 most discriminatory terms."
      ],
      "metadata": {
        "id": "taL2cslr2YN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from six import text_type\n",
        "text_representation = tree.export_text(model_tree, feature_names=v.get_feature_names())\n",
        "print(text_representation)"
      ],
      "metadata": {
        "id": "cYOkM8FR11vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"3 most discriminatory terms are: 'thing', 'ireland' oraz 'avoid' (based on rules analysis)\")"
      ],
      "metadata": {
        "id": "VMzyG46k7io0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}